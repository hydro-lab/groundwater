#!/usr/bin/python

# Usage:
#
#    In a terminal/command line, cd to the directory where this file lives. Then...
#
#    With embedded urls: ( download the hardcoded list of files in the 'files =' block below)
#
#       python ./download-all-2021-09-28_14-22-13.py
#
#    Download all files in a Metalink/CSV: (downloaded from ASF Vertex)
#
#       python ./download-all-2021-09-28_14-22-13.py /path/to/downloads.metalink localmetalink.metalink localcsv.csv
#
#    Compatibility: python >= 2.6.5, 2.7.5, 3.0
#
#    If downloading from a trusted source with invalid SSL Certs, use --insecure to ignore
#
#    For more information on bulk downloads, navigate to:
#        https://asf.alaska.edu/how-to/data-tools/data-tools/#bulk_download
#
#
#
#    This script was generated by the Alaska Satellite Facility's bulk download service.
#    For more information on the service, navigate to:
#        http://bulk-download.asf.alaska.edu/help
#

import sys, csv
import os, os.path
import tempfile, shutil
import re

import base64
import time
import getpass
import ssl
import signal
import socket

import xml.etree.ElementTree as ET

#############
# This next block is a bunch of Python 2/3 compatability

try:
   # Python 2.x Libs
   from urllib2 import build_opener, install_opener, Request, urlopen, HTTPError
   from urllib2 import URLError, HTTPSHandler,  HTTPHandler, HTTPCookieProcessor

   from cookielib import MozillaCookieJar
   from StringIO import StringIO

except ImportError as e:

   # Python 3.x Libs
   from urllib.request import build_opener, install_opener, Request, urlopen
   from urllib.request import HTTPHandler, HTTPSHandler, HTTPCookieProcessor
   from urllib.error import HTTPError, URLError

   from http.cookiejar import MozillaCookieJar
   from io import StringIO

###
# Global variables intended for cross-thread modification
abort = False

###
# A routine that handles trapped signals
def signal_handler(sig, frame):
    global abort
    sys.stderr.output("\n > Caught Signal. Exiting!\n")
    abort = True # necessary to cause the program to stop
    raise SystemExit  # this will only abort the thread that the ctrl+c was caught in

class bulk_downloader:
    def __init__(self):
        # List of files to download
        self.files = [ "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160520T162211_20160520T162237_011344_01137D_3359.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160707T162213_20160707T162240_012044_0129D9_BDD5.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160601T162208_20160601T162235_011519_011933_A061.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160719T162211_20160719T162238_012219_012F8A_3CC8.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160613T162212_20160613T162239_011694_011EAC_64A9.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160731T162215_20160731T162242_012394_01354A_5DB1.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160917T162217_20160917T162244_013094_014C71_93B4.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160812T162213_20160812T162239_012569_013B18_EB3F.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160929T162214_20160929T162241_013269_015225_56EB.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160824T162216_20160824T162239_012744_0140F7_2555.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20161011T162214_20161011T162241_013444_0157AC_CE80.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170608T162215_20170608T162242_016944_01C340_A911.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170807T162218_20170807T162245_017819_01DDE8_6CAF.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170620T162215_20170620T162242_017119_01C8A6_D36C.zip",
                       "https://datapool.asf.alaska.edu/SLC/SB/S1B_IW_SLC__1SDV_20170801T162136_20170801T162203_006748_00BDFF_5857.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170702T162216_20170702T162243_017294_01CDED_4BC0.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170819T162219_20170819T162246_017994_01E335_66E7.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180215T162217_20180215T162244_020619_0234D6_3E8F.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180323T162217_20180323T162244_021144_02457D_AEBE.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200124T162230_20200124T162257_030944_038D7D_7F6D.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200312T162230_20200312T162257_031644_03A5C0_0342.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200205T162230_20200205T162257_031119_03939C_0437.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200324T162230_20200324T162257_031819_03ABF2_5FC4.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200217T162230_20200217T162257_031294_0399AB_6357.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200405T162230_20200405T162257_031994_03B213_AF01.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201201T162239_20201201T162306_035494_042651_2F57.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20210118T162237_20210118T162304_036194_043E94_8E64.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201213T162238_20201213T162305_035669_042C4E_62B4.zip",
                       "https://datapool.asf.alaska.edu/SLC/SB/S1B_IW_SLC__1SDV_20210124T162159_20210124T162229_025298_03034B_A675.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201225T162238_20201225T162305_035844_043255_376F.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20210223T162236_20210223T162303_036719_0450D2_85C1.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160109T162206_20160109T162233_009419_00DA5D_CBEF.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160121T162205_20160121T162232_009594_00DF6D_C870.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160202T162205_20160202T162232_009769_00E486_B20B.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160214T162216_20160214T162239_009944_00E99A_504D.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160226T162205_20160226T162232_010119_00EEB7_3B05.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160309T162205_20160309T162232_010294_00F3AC_2403.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160321T162205_20160321T162232_010469_00F8A2_A229.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160402T162206_20160402T162233_010644_00FDA6_C072.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160414T162206_20160414T162233_010819_0102EB_C9B9.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160426T162207_20160426T162233_010994_01084D_27A0.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20160508T162207_20160508T162234_011169_010DD4_22A4.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20161023T162214_20161023T162241_013619_015D27_F399.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20161104T162214_20161104T162241_013794_01629E_DDAE.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20161116T162214_20161116T162241_013969_01680E_F997.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20161128T162214_20161128T162241_014144_016D6E_F457.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20161210T162214_20161210T162240_014319_0172FD_EC94.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20161222T162213_20161222T162240_014494_017871_CECD.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170103T162211_20170103T162238_014669_017DCD_4895.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170115T162211_20170115T162238_014844_01832C_67B4.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170127T162211_20170127T162238_015019_018890_F9D6.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170208T162211_20170208T162238_015194_018E03_E827.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170220T162210_20170220T162237_015369_019374_4D3D.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170304T162210_20170304T162237_015544_0198BD_7E62.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170316T162211_20170316T162238_015719_019DF8_98A8.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170328T162211_20170328T162238_015894_01A336_0E1A.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170409T162211_20170409T162238_016069_01A874_C634.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170421T162212_20170421T162239_016244_01ADCF_0F82.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170503T162213_20170503T162240_016419_01B31D_CEBE.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170515T162213_20170515T162240_016594_01B870_9284.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170527T162214_20170527T162241_016769_01BDD2_3658.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170831T162219_20170831T162246_018169_01E878_1D33.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170912T162220_20170912T162246_018344_01EDF5_FBE9.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20170924T162220_20170924T162247_018519_01F350_D440.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171006T162220_20171006T162247_018694_01F8A5_6871.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171018T162220_20171018T162247_018869_01FE01_AAD2.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171030T162220_20171030T162247_019044_020353_EE26.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171111T162220_20171111T162247_019219_0208B6_470D.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171123T162220_20171123T162247_019394_020E3D_71D7.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171205T162219_20171205T162246_019569_0213BB_2E32.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171217T162219_20171217T162246_019744_02192B_8EA7.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20171229T162218_20171229T162245_019919_021E96_5060.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180110T162218_20180110T162245_020094_02241D_F16A.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180122T162218_20180122T162245_020269_0229AB_A0F1.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180203T162217_20180203T162244_020444_022F42_3A7E.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180227T162217_20180227T162244_020794_023A70_CFE0.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180311T162217_20180311T162244_020969_023FEE_2A63.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180404T162217_20180404T162244_021319_024B03_8986.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180416T162218_20180416T162245_021494_025077_11CE.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180428T162219_20180428T162246_021669_0255F0_885C.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180510T162219_20180510T162246_021844_025B7F_7487.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180522T162220_20180522T162247_022019_026112_9BD3.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180603T162220_20180603T162247_022194_02669B_74FE.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180615T162221_20180615T162248_022369_026C0A_AF8C.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180627T162222_20180627T162249_022544_02712E_1FA5.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180709T162223_20180709T162250_022719_027649_B93D.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180721T162223_20180721T162250_022894_027BB1_6267.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180802T162224_20180802T162251_023069_02812F_581C.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180814T162225_20180814T162252_023244_0286D2_0E0F.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180826T162226_20180826T162252_023419_028C6F_FFB6.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180907T162226_20180907T162253_023594_029206_782E.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20180919T162226_20180919T162253_023769_0297A7_DBEB.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181001T162227_20181001T162254_023944_029D5B_448E.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181013T162227_20181013T162254_024119_02A314_A14E.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181025T162227_20181025T162254_024294_02A8BB_91FF.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181106T162227_20181106T162254_024469_02AEC5_3288.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181118T162227_20181118T162254_024644_02B534_905D.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181130T162226_20181130T162253_024819_02BB76_5C1B.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181212T162225_20181212T162252_024994_02C181_8C57.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20181224T162225_20181224T162252_025169_02C7D4_774C.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190105T162225_20190105T162252_025344_02CE22_7903.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190117T162224_20190117T162251_025519_02D472_C632.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190129T162224_20190129T162251_025694_02DADC_33B6.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190210T162224_20190210T162250_025869_02E127_5513.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190222T162223_20190222T162250_026044_02E75A_0133.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190306T162224_20190306T162250_026219_02ED9C_A8BB.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190318T162223_20190318T162250_026394_02F414_25C3.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190330T162224_20190330T162251_026569_02FA83_2C82.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190411T162224_20190411T162251_026744_0300EC_58EC.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190423T162225_20190423T162252_026919_030740_6497.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190505T162225_20190505T162252_027094_030DA6_AF5E.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190517T162226_20190517T162253_027269_03132A_8572.zip",
                       "https://datapool.asf.alaska.edu/SLC/SB/S1B_IW_SLC__1SDV_20190523T162146_20190523T162222_016373_01ED2E_F028.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190529T162226_20190529T162253_027444_0318A2_F0C9.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190610T162227_20190610T162254_027619_031DFE_C299.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190622T162228_20190622T162255_027794_03233A_7762.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190704T162228_20190704T162255_027969_032882_7859.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190716T162229_20190716T162256_028144_032DD1_A702.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190728T162230_20190728T162257_028319_033328_C3D4.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190809T162231_20190809T162258_028494_033894_7ACA.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190821T162231_20190821T162258_028669_033EAE_7635.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190902T162232_20190902T162259_028844_0344C4_9965.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190914T162232_20190914T162259_029019_034AD9_08B3.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20190926T162233_20190926T162300_029194_0350C9_85D8.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191008T162233_20191008T162300_029369_0356D5_A22F.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191020T162233_20191020T162300_029544_035CE1_9345.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191101T162233_20191101T162300_029719_0362F6_91C6.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191113T162233_20191113T162300_029894_03691B_BED6.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191125T162233_20191125T162300_030069_036F2C_5E35.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191207T162232_20191207T162259_030244_03752D_0A75.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191219T162232_20191219T162259_030419_037B3B_CDEE.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20191231T162231_20191231T162258_030594_038146_BBE6.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200112T162231_20200112T162258_030769_038759_BD67.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200417T162231_20200417T162258_032169_03B842_1A48.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200429T162231_20200429T162258_032344_03BE64_0E65.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200511T162232_20200511T162259_032519_03C42D_80B0.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200523T162233_20200523T162300_032694_03C97A_4B88.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200604T162233_20200604T162300_032869_03CEAA_A2F9.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200616T162234_20200616T162301_033044_03D3EC_D0B9.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200628T162235_20200628T162302_033219_03D93C_6A13.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200710T162235_20200710T162302_033394_03DE8A_DFE3.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200722T162236_20200722T162303_033569_03E3EA_BB97.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200803T162237_20200803T162304_033744_03E941_24EF.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200815T162238_20200815T162305_033919_03EF4A_EEE0.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200827T162238_20200827T162305_034094_03F573_601E.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200908T162239_20200908T162306_034269_03FB9C_566F.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20200920T162239_20200920T162306_034444_0401DA_62C3.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201002T162239_20201002T162306_034619_0407FC_BAA1.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201014T162240_20201014T162307_034794_040E17_78C3.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201026T162240_20201026T162307_034969_041419_CA97.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201107T162239_20201107T162306_035144_041A23_7917.zip",
                       "https://datapool.asf.alaska.edu/SLC/SA/S1A_IW_SLC__1SDV_20201119T162239_20201119T162306_035319_04203B_683F.zip" ]

        # Local stash of cookies so we don't always have to ask
        self.cookie_jar_path = os.path.join( os.path.expanduser('~'), ".bulk_download_cookiejar.txt")
        self.cookie_jar = None

        self.asf_urs4 = { 'url': 'https://urs.earthdata.nasa.gov/oauth/authorize',
                 'client': 'BO_n7nTIlMljdvU6kRRB3g',
                 'redir': 'https://auth.asf.alaska.edu/login'}

        # Make sure we can write it our current directory
        if os.access(os.getcwd(), os.W_OK) is False:
            print ("WARNING: Cannot write to current path! Check permissions for {0}".format(os.getcwd()))
            exit(-1)

        # For SSL
        self.context = {}

        # Check if user handed in a Metalink or CSV:
        if len(sys.argv) > 0:
            download_files = []
            input_files = []
            for arg in sys.argv[1:]:
                if arg == '--insecure':
                    try:
                        ctx = ssl.create_default_context()
                        ctx.check_hostname = False
                        ctx.verify_mode = ssl.CERT_NONE
                        self.context['context'] = ctx
                    except AttributeError:
                        # Python 2.6 won't complain about SSL Validation
                        pass

                elif arg.endswith('.metalink') or arg.endswith('.csv'):
                    if os.path.isfile( arg ):
                        input_files.append( arg )
                        if arg.endswith('.metalink'):
                            new_files = self.process_metalink(arg)
                        else:
                            new_files = self.process_csv(arg)
                        if new_files is not None:
                            for file_url in (new_files):
                                download_files.append( file_url )
                    else:
                         print (" > I cannot find the input file you specified: {0}".format(arg))
                else:
                    print (" > Command line argument '{0}' makes no sense, ignoring.".format(arg))

            if len(input_files) > 0:
                if len(download_files) > 0:
                    print (" > Processing {0} downloads from {1} input files. ".format(len(download_files), len(input_files)))
                    self.files = download_files
                else:
                    print (" > I see you asked me to download files from {0} input files, but they had no downloads!".format(len(input_files)))
                    print (" > I'm super confused and exiting.")
                    exit(-1)

        # Make sure cookie_jar is good to go!
        self.get_cookie()

         # summary
        self.total_bytes = 0
        self.total_time = 0
        self.cnt = 0
        self.success = []
        self.failed = []
        self.skipped = []


    # Get and validate a cookie
    def get_cookie(self):
       if os.path.isfile(self.cookie_jar_path):
          self.cookie_jar = MozillaCookieJar()
          self.cookie_jar.load(self.cookie_jar_path)

          # make sure cookie is still valid
          if self.check_cookie():
             print(" > Reusing previous cookie jar.")
             return True
          else:
             print(" > Could not validate old cookie Jar")

       # We don't have a valid cookie, prompt user or creds
       print ("No existing URS cookie found, please enter Earthdata username & password:")
       print ("(Credentials will not be stored, saved or logged anywhere)")

       # Keep trying 'till user gets the right U:P
       while self.check_cookie() is False:
          self.get_new_cookie()

       return True

    # Validate cookie before we begin
    def check_cookie(self):

       if self.cookie_jar is None:
          print (" > Cookiejar is bunk: {0}".format(self.cookie_jar))
          return False

       # File we know is valid, used to validate cookie
       file_check = 'https://urs.earthdata.nasa.gov/profile'

       # Apply custom Redirect Hanlder
       opener = build_opener(HTTPCookieProcessor(self.cookie_jar), HTTPHandler(), HTTPSHandler(**self.context))
       install_opener(opener)

       # Attempt a HEAD request
       request = Request(file_check)
       request.get_method = lambda : 'HEAD'
       try:
          print (" > attempting to download {0}".format(file_check))
          response = urlopen(request, timeout=30)
          resp_code = response.getcode()
          # Make sure we're logged in
          if not self.check_cookie_is_logged_in(self.cookie_jar):
             return False

          # Save cookiejar
          self.cookie_jar.save(self.cookie_jar_path)

       except HTTPError:
          # If we ge this error, again, it likely means the user has not agreed to current EULA
          print ("\nIMPORTANT: ")
          print ("Your user appears to lack permissions to download data from the ASF Datapool.")
          print ("\n\nNew users: you must first log into Vertex and accept the EULA. In addition, your Study Area must be set at Earthdata https://urs.earthdata.nasa.gov")
          exit(-1)

       # This return codes indicate the USER has not been approved to download the data
       if resp_code in (300, 301, 302, 303):
          try:
             redir_url = response.info().getheader('Location')
          except AttributeError:
             redir_url = response.getheader('Location')

          #Funky Test env:
          if ("vertex-retired.daac.asf.alaska.edu" in redir_url and "test" in self.asf_urs4['redir']):
             print ("Cough, cough. It's dusty in this test env!")
             return True

          print ("Redirect ({0}) occured, invalid cookie value!".format(resp_code))
          return False

       # These are successes!
       if resp_code in (200, 307):
          return True

       return False

    def get_new_cookie(self):
       # Start by prompting user to input their credentials

       # Another Python2/3 workaround
       try:
          new_username = raw_input("Username: ")
       except NameError:
          new_username = input("Username: ")
       new_password = getpass.getpass(prompt="Password (will not be displayed): ")

       # Build URS4 Cookie request
       auth_cookie_url = self.asf_urs4['url'] + '?client_id=' + self.asf_urs4['client'] + '&redirect_uri=' + self.asf_urs4['redir'] + '&response_type=code&state='

       try:
          #python2
          user_pass = base64.b64encode (bytes(new_username+":"+new_password))
       except TypeError:
          #python3
          user_pass = base64.b64encode (bytes(new_username+":"+new_password, "utf-8"))
          user_pass = user_pass.decode("utf-8")

       # Authenticate against URS, grab all the cookies
       self.cookie_jar = MozillaCookieJar()
       opener = build_opener(HTTPCookieProcessor(self.cookie_jar), HTTPHandler(), HTTPSHandler(**self.context))
       request = Request(auth_cookie_url, headers={"Authorization": "Basic {0}".format(user_pass)})

       # Watch out cookie rejection!
       try:
          response = opener.open(request)
       except HTTPError as e:
          if "WWW-Authenticate" in e.headers and "Please enter your Earthdata Login credentials" in e.headers["WWW-Authenticate"]:
             print (" > Username and Password combo was not successful. Please try again.")
             return False
          else:
             # If an error happens here, the user most likely has not confirmed EULA.
             print ("\nIMPORTANT: There was an error obtaining a download cookie!")
             print ("Your user appears to lack permission to download data from the ASF Datapool.")
             print ("\n\nNew users: you must first log into Vertex and accept the EULA. In addition, your Study Area must be set at Earthdata https://urs.earthdata.nasa.gov")
             exit(-1)
       except URLError as e:
          print ("\nIMPORTANT: There was a problem communicating with URS, unable to obtain cookie. ")
          print ("Try cookie generation later.")
          exit(-1)

       # Did we get a cookie?
       if self.check_cookie_is_logged_in(self.cookie_jar):
          #COOKIE SUCCESS!
          self.cookie_jar.save(self.cookie_jar_path)
          return True

       # if we aren't successful generating the cookie, nothing will work. Stop here!
       print ("WARNING: Could not generate new cookie! Cannot proceed. Please try Username and Password again.")
       print ("Response was {0}.".format(response.getcode()))
       print ("\n\nNew users: you must first log into Vertex and accept the EULA. In addition, your Study Area must be set at Earthdata https://urs.earthdata.nasa.gov")
       exit(-1)

    # make sure we're logged into URS
    def check_cookie_is_logged_in(self, cj):
       for cookie in cj:
          if cookie.name == 'urs_user_already_logged':
              # Only get this cookie if we logged in successfully!
              return True

       return False


    # Download the file
    def download_file_with_cookiejar(self, url, file_count, total, recursion=False):
       # see if we've already download this file and if it is that it is the correct size
       download_file = os.path.basename(url).split('?')[0]
       if os.path.isfile(download_file):
          try:
             request = Request(url)
             request.get_method = lambda : 'HEAD'
             response = urlopen(request, timeout=30)
             remote_size = self.get_total_size(response)
             # Check that we were able to derive a size.
             if remote_size:
                 local_size = os.path.getsize(download_file)
                 if remote_size < (local_size+(local_size*.01)) and remote_size > (local_size-(local_size*.01)):
                     print (" > Download file {0} exists! \n > Skipping download of {1}. ".format(download_file, url))
                     return None,None
                 #partial file size wasn't full file size, lets blow away the chunk and start again
                 print (" > Found {0} but it wasn't fully downloaded. Removing file and downloading again.".format(download_file))
                 os.remove(download_file)

          except ssl.CertificateError as e:
             print (" > ERROR: {0}".format(e))
             print (" > Could not validate SSL Cert. You may be able to overcome this using the --insecure flag")
             return False,None

          except HTTPError as e:
             if e.code == 401:
                 print (" > IMPORTANT: Your user may not have permission to download this type of data!")
             else:
                 print (" > Unknown Error, Could not get file HEAD: {0}".format(e))

          except URLError as e:
             print ("URL Error (from HEAD): {0}, {1}".format( e.reason, url))
             if "ssl.c" in "{0}".format(e.reason):
                 print ("IMPORTANT: Remote location may not be accepting your SSL configuration. This is a terminal error.")
             return False,None

       # attempt https connection
       try:
          request = Request(url)
          response = urlopen(request, timeout=30)

          # Watch for redirect
          if response.geturl() != url:

             # See if we were redirect BACK to URS for re-auth.
             if 'https://urs.earthdata.nasa.gov/oauth/authorize' in response.geturl():

                 if recursion:
                     print (" > Entering seemingly endless auth loop. Aborting. ")
                     return False, None

                 # make this easier. If there is no app_type=401, add it
                 new_auth_url = response.geturl()
                 if "app_type" not in new_auth_url:
                     new_auth_url += "&app_type=401"

                 print (" > While attempting to download {0}....".format(url))
                 print (" > Need to obtain new cookie from {0}".format(new_auth_url))
                 old_cookies = [cookie.name for cookie in self.cookie_jar]
                 opener = build_opener(HTTPCookieProcessor(self.cookie_jar), HTTPHandler(), HTTPSHandler(**self.context))
                 request = Request(new_auth_url)
                 try:
                     response = opener.open(request)
                     for cookie in self.cookie_jar:
                         if cookie.name not in old_cookies:
                              print (" > Saved new cookie: {0}".format(cookie.name))

                              # A little hack to save session cookies
                              if cookie.discard:
                                   cookie.expires = int(time.time()) + 60*60*24*30
                                   print (" > Saving session Cookie that should have been discarded! ")

                     self.cookie_jar.save(self.cookie_jar_path, ignore_discard=True, ignore_expires=True)
                 except HTTPError as e:
                     print ("HTTP Error: {0}, {1}".format( e.code, url))
                     return False,None

                 # Okay, now we have more cookies! Lets try again, recursively!
                 print (" > Attempting download again with new cookies!")
                 return self.download_file_with_cookiejar(url, file_count, total, recursion=True)

             print (" > 'Temporary' Redirect download @ Remote archive:\n > {0}".format(response.geturl()))

          # seems to be working
          print ("({0}/{1}) Downloading {2}".format(file_count, total, url))

          # Open our local file for writing and build status bar
          tf = tempfile.NamedTemporaryFile(mode='w+b', delete=False, dir='.')
          self.chunk_read(response, tf, report_hook=self.chunk_report)

          # Reset download status
          sys.stdout.write('\n')

          tempfile_name = tf.name
          tf.close()

       #handle errors
       except HTTPError as e:
          print ("HTTP Error: {0}, {1}".format( e.code, url))

          if e.code == 401:
             print (" > IMPORTANT: Your user does not have permission to download this type of data!")

          if e.code == 403:
             print (" > Got a 403 Error trying to download this file.  ")
             print (" > You MAY need to log in this app and agree to a EULA. ")

          return False,None

       except URLError as e:
          print ("URL Error (from GET): {0}, {1}, {2}".format(e, e.reason, url))
          if "ssl.c" in "{0}".format(e.reason):
              print ("IMPORTANT: Remote location may not be accepting your SSL configuration. This is a terminal error.")
          return False,None

       except socket.timeout as e:
           print (" > timeout requesting: {0}; {1}".format(url, e))
           return False,None

       except ssl.CertificateError as e:
          print (" > ERROR: {0}".format(e))
          print (" > Could not validate SSL Cert. You may be able to overcome this using the --insecure flag")
          return False,None

       # Return the file size
       shutil.copy(tempfile_name, download_file)
       os.remove(tempfile_name)
       file_size = self.get_total_size(response)
       actual_size = os.path.getsize(download_file)
       if file_size is None:
           # We were unable to calculate file size.
           file_size = actual_size
       return actual_size,file_size

    def get_redirect_url_from_error(self, error):
       find_redirect = re.compile(r"id=\"redir_link\"\s+href=\"(\S+)\"")
       print ("error file was: {}".format(error))
       redirect_url = find_redirect.search(error)
       if redirect_url:
          print("Found: {0}".format(redirect_url.group(0)))
          return (redirect_url.group(0))

       return None


    #  chunk_report taken from http://stackoverflow.com/questions/2028517/python-urllib2-progress-hook
    def chunk_report(self, bytes_so_far, file_size):
       if file_size is not None:
           percent = float(bytes_so_far) / file_size
           percent = round(percent*100, 2)
           sys.stdout.write(" > Downloaded %d of %d bytes (%0.2f%%)\r" %
               (bytes_so_far, file_size, percent))
       else:
           # We couldn't figure out the size.
           sys.stdout.write(" > Downloaded %d of unknown Size\r" % (bytes_so_far))

    #  chunk_read modified from http://stackoverflow.com/questions/2028517/python-urllib2-progress-hook
    def chunk_read(self, response, local_file, chunk_size=8192, report_hook=None):
       file_size = self.get_total_size(response)
       bytes_so_far = 0

       while 1:
          try:
             chunk = response.read(chunk_size)
          except:
             sys.stdout.write("\n > There was an error reading data. \n")
             break

          try:
             local_file.write(chunk)
          except TypeError:
             local_file.write(chunk.decode(local_file.encoding))
          bytes_so_far += len(chunk)

          if not chunk:
             break

          if report_hook:
             report_hook(bytes_so_far, file_size)

       return bytes_so_far

    def get_total_size(self, response):
       try:
          file_size = response.info().getheader('Content-Length').strip()
       except AttributeError:
          try:
             file_size = response.getheader('Content-Length').strip()
          except AttributeError:
             print ("> Problem getting size")
             return None

       return int(file_size)


    # Get download urls from a metalink file
    def process_metalink(self, ml_file):
       print ("Processing metalink file: {0}".format(ml_file))
       with open(ml_file, 'r') as ml:
          xml = ml.read()

       # Hack to remove annoying namespace
       it = ET.iterparse(StringIO(xml))
       for _, el in it:
          if '}' in el.tag:
             el.tag = el.tag.split('}', 1)[1]  # strip all namespaces
       root = it.root

       dl_urls = []
       ml_files = root.find('files')
       for dl in ml_files:
          dl_urls.append(dl.find('resources').find('url').text)

       if len(dl_urls) > 0:
          return dl_urls
       else:
          return None

    # Get download urls from a csv file
    def process_csv(self, csv_file):
       print ("Processing csv file: {0}".format(csv_file))

       dl_urls = []
       with open(csv_file, 'r') as csvf:
          try:
             csvr = csv.DictReader(csvf)
             for row in csvr:
                dl_urls.append(row['URL'])
          except csv.Error as e:
             print ("WARNING: Could not parse file %s, line %d: %s. Skipping." % (csv_file, csvr.line_num, e))
             return None
          except KeyError as e:
             print ("WARNING: Could not find URL column in file %s. Skipping." % (csv_file))

       if len(dl_urls) > 0:
          return dl_urls
       else:
          return None

    # Download all the files in the list
    def download_files(self):
        for file_name in self.files:

            # make sure we haven't ctrl+c'd or some other abort trap
            if abort == True:
              raise SystemExit

            # download counter
            self.cnt += 1

            # set a timer
            start = time.time()

            # run download
            size,total_size = self.download_file_with_cookiejar(file_name, self.cnt, len(self.files))

            # calculte rate
            end = time.time()

            # stats:
            if size is None:
                self.skipped.append(file_name)
            # Check to see that the download didn't error and is the correct size
            elif size is not False and (total_size < (size+(size*.01)) and total_size > (size-(size*.01))):
                # Download was good!
                elapsed = end - start
                elapsed = 1.0 if elapsed < 1 else elapsed
                rate = (size/1024**2)/elapsed

                print ("Downloaded {0}b in {1:.2f}secs, Average Rate: {2:.2f}MB/sec".format(size, elapsed, rate))

                # add up metrics
                self.total_bytes += size
                self.total_time += elapsed
                self.success.append( {'file':file_name, 'size':size } )

            else:
                print ("There was a problem downloading {0}".format(file_name))
                self.failed.append(file_name)

    def print_summary(self):
        # Print summary:
        print ("\n\nDownload Summary ")
        print ("--------------------------------------------------------------------------------")
        print ("  Successes: {0} files, {1} bytes ".format(len(self.success), self.total_bytes))
        for success_file in self.success:
           print ("           - {0}  {1:.2f}MB".format(success_file['file'],(success_file['size']/1024.0**2)))
        if len(self.failed) > 0:
           print ("  Failures: {0} files".format(len(self.failed)))
           for failed_file in self.failed:
              print ("          - {0}".format(failed_file))
        if len(self.skipped) > 0:
           print ("  Skipped: {0} files".format(len(self.skipped)))
           for skipped_file in self.skipped:
              print ("          - {0}".format(skipped_file))
        if len(self.success) > 0:
           print ("  Average Rate: {0:.2f}MB/sec".format( (self.total_bytes/1024.0**2)/self.total_time))
        print ("--------------------------------------------------------------------------------")


if __name__ == "__main__":
    # Setup a signal trap for SIGINT (Ctrl+C)
    signal.signal(signal.SIGINT, signal_handler)

    downloader = bulk_downloader()
    downloader.download_files()
    downloader.print_summary()
